{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08cdb11d-7897-42da-8f68-ce1a70a8af02",
   "metadata": {},
   "source": [
    "1.An ensemble technique in machine learning combines the predictions from multiple models to produce a more robust and accurate prediction than any single model. The idea is to leverage the strengths and mitigate the weaknesses of individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267877b7-2680-4bc7-abd8-963686a80fc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "2.Ensemble techniques are used because they:\n",
    "\n",
    "Improve predictive performance by reducing the variance, bias, or both.\n",
    "Increase the robustness of the model.\n",
    "Enhance the stability of the predictions.\n",
    "Help to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca6ae3-a107-4116-95c2-2c316375d193",
   "metadata": {},
   "source": [
    "3.Bagging (Bootstrap Aggregating) is an ensemble technique that involves training multiple versions of a model on different subsets of the training data (created by bootstrapping) and then averaging their predictions for regression tasks or using a majority vote for classification tasks. It helps in reducing the variance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d176f-fb2d-490c-b504-cce5d7c4d372",
   "metadata": {},
   "source": [
    "4.Boosting is an ensemble technique that combines the predictions of several base models (usually weak learners) to form a strong learner. Models are trained sequentially, and each new model attempts to correct the errors of its predecessor by focusing more on the previously misclassified instances. Examples of boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd9eee-8f53-40c2-b08a-31565bf6882e",
   "metadata": {},
   "source": [
    "5.The benefits of using ensemble techniques include:\n",
    "\n",
    "Improved predictive accuracy.\n",
    "Reduced risk of overfitting.\n",
    "Enhanced model generalizability.\n",
    "Increased robustness to noise and outliers.\n",
    "Better performance on complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489bca08-4cbf-4a29-a087-c443fd4acfe6",
   "metadata": {},
   "source": [
    "6.Ensemble techniques are generally more robust and accurate than individual models. However, they are not always better in every scenario. Their performance depends on:\n",
    "\n",
    "The diversity and strength of the base models.\n",
    "The nature of the data.\n",
    "Computational resources, as ensemble methods are typically more resource-intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417b113-7e23-45e4-954d-6aae05adee75",
   "metadata": {},
   "source": [
    "7.The confidence interval using bootstrap is calculated by:\n",
    "\n",
    "Drawing multiple bootstrap samples from the original data.\n",
    "Computing the statistic of interest (e.g., mean) for each bootstrap sample.\n",
    "Constructing the empirical distribution of the statistic from these bootstrap samples.\n",
    "Determining the confidence interval by finding the appropriate percentiles from the bootstrap distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac06e2-7a2e-4204-914f-1832c228a7c9",
   "metadata": {},
   "source": [
    "8.Bootstrap works by resampling the original data with replacement to create multiple bootstrap samples. The steps involved are:\n",
    "\n",
    "Draw a large number of bootstrap samples from the original dataset (each of the same size as the original dataset).\n",
    "Calculate the statistic of interest (e.g., mean, variance) for each bootstrap sample.\n",
    "Construct the empirical distribution of the statistic from these bootstrap samples.\n",
    "Use the empirical distribution to estimate the confidence interval or other statistical properties of the statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a37c36-c675-4977-80c0-967d70e9bcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the population mean height: (14.03, 15.06)\n"
     ]
    }
   ],
   "source": [
    "9.#To estimate the 95% confidence interval using bootstrap, follow these steps:\n",
    "\n",
    "#Generate Bootstrap Samples:\n",
    "#Resample the data with replacement to create a large number (e.g., 1000 or 10,000) of bootstrap samples.\n",
    "\n",
    "#Calculate the Mean for Each Bootstrap Sample:\n",
    "#Compute the mean height for each bootstrap sample.\n",
    "\n",
    "#Construct the Empirical Distribution:\n",
    "#Use the distribution of bootstrap means to estimate the confidence interval.\n",
    "import numpy as np\n",
    "\n",
    "# Original sample mean and standard deviation\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "n = 50\n",
    "\n",
    "# Generate a sample of 50 tree heights assuming a normal distribution\n",
    "np.random.seed(42)\n",
    "original_sample = np.random.normal(sample_mean, sample_std, n)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 10000\n",
    "\n",
    "# Generate bootstrap samples and calculate their means\n",
    "bootstrap_means = np.empty(n_bootstraps)\n",
    "for i in range(n_bootstraps):\n",
    "    bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f'95% Confidence Interval for the population mean height: ({lower_bound:.2f}, {upper_bound:.2f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a85560-89ee-442b-a870-60f72d0d5971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
